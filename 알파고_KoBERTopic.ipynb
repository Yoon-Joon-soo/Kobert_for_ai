{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzUe/wImo3njc+fZ7e5ITQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoon-Joon-soo/StyleGan/blob/master/%EC%95%8C%ED%8C%8C%EA%B3%A0_KoBERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sLu_hQWWFKy7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/sample_data/1. 1위 작품, 알고보니 AI가 그린 그림…예술품일까 논란  SBS  생생지구촌.csv',encoding='cp949')\n",
        "docs1 = df1['comment']\n",
        "docs1\n",
        "\n",
        "df2 = pd.read_csv('/content/sample_data/2. AI 창작물의 저작권은 최초 인정받은 작가의 말 이슈탐사 SBS.csv',encoding='cp949')\n",
        "docs2 = df2['comment']\n",
        "docs2\n",
        "\n",
        "df3 = pd.read_csv('/content/sample_data/3. AI가 그려도 저작권 인정…원작자 예술 기여 넓어져 SBS.csv',encoding='cp949')\n",
        "docs3 = df3['comment']\n",
        "docs3\n",
        "\n",
        "df4 = pd.read_csv('/content/sample_data/4. AI가 그린 그림이 미술대회 1등해서 난리난 미술계 근황 스브스뉴스.csv',encoding='cp949')\n",
        "docs4 = df4['comment']\n",
        "docs4\n",
        "\n",
        "df5 = pd.read_csv('/content/sample_data/5. AI가 그린 만화, 저작권 승인받았다…저작권 전쟁 본격화 SBS 오클릭.csv',encoding='cp949')\n",
        "docs5 = df5['comment']\n",
        "docs5\n",
        "\n",
        "df6 = pd.read_csv('/content/sample_data/6. 그림체 도둑질당한 작가들이 내 그림 지키려 나선 이유 스브스뉴스.csv',encoding='cp949')\n",
        "docs6 = df6['comment']\n",
        "docs6\n",
        "\n",
        "df7 = pd.read_csv('/content/sample_data/7. 대상 받은 AI도 나왔지만…특허청 발명자는 될 수 없다 SBS.csv',encoding='cp949')\n",
        "docs7 = df7['comment']\n",
        "docs7\n",
        "\n",
        "df8 = pd.read_csv('/content/sample_data/8. 발명했지만 발명자 아니다…인공지능에 선그은 특허청 SBS.csv',encoding='cp949')\n",
        "docs8 = df8['comment']\n",
        "docs8\n",
        "\n",
        "df9 = pd.read_csv('/content/sample_data/9. 사진대회 뒤흔든 폭로…상 안 받을래, 사실 AI 작품이라서 SBS 실시간 e뉴스.csv',encoding='cp949')\n",
        "docs9 = df9['comment']\n",
        "docs9\n",
        "\n",
        "df10 = pd.read_csv('/content/sample_data/10. 미술대회서 인공지능 작품이 1등…붓질 없어도 예술 뉴스A.csv',encoding='cp949')\n",
        "docs10 = df10['comment']\n",
        "docs10\n",
        "\n",
        "df11 = pd.read_csv('/content/sample_data/11. 과학 한스푼 AI가 그린 만화책은 어떤 모습 저작권·일자리 논쟁 가열 YTN.csv',encoding='cp949')\n",
        "docs11 = df11['comment']\n",
        "docs11\n",
        "\n",
        "df12 = pd.read_csv('/content/sample_data/12. 국제사진전 수상 뒤 사실은 AI가 만든 이미지 고백  YTN.csv',encoding='cp949')\n",
        "docs12 = df12['comment']\n",
        "docs12\n",
        "\n",
        "df13 = pd.read_csv('/content/sample_data/13. 그림 그리는 AI...웹툰계 판도 흔든다 YTN.csv',encoding='cp949')\n",
        "docs13 = df13['comment']\n",
        "docs13\n",
        "\n",
        "df14 = pd.read_csv('/content/sample_data/14. 그림 도둑 잡아라! AI에 밥그릇 뺏긴 예술가들 YTN.csv',encoding='cp949')\n",
        "docs14 = df14['comment']\n",
        "docs14\n",
        "\n",
        "df15 = pd.read_csv('/content/sample_data/15. 그림 도둑 잡아라! AI에 밥그릇 뺏긴 예술가들 YTN2.csv',encoding='cp949')\n",
        "docs15 = df15['comment']\n",
        "docs15\n",
        "\n",
        "df16 = pd.read_csv('/content/sample_data/16. (글로벌K) 미 미술전서 AI가 그린 그림이 1위…논란 일파만파 KBS 2022.09.07.csv',encoding='cp949')\n",
        "docs16 = df16['comment']\n",
        "docs16\n",
        "\n",
        "df17 = pd.read_csv('/content/sample_data/17. AI가 만든 그림 보고 음악 듣지만…‘권리’와 ‘책임’ 논의는 제자리 (9시 뉴스)  KBS 2023.01.24.csv',encoding='cp949')\n",
        "docs17 = df17['comment']\n",
        "docs17\n",
        "\n",
        "df18 = pd.read_csv('/content/sample_data/18.(집중취재M) AI가 그린 이현세의 까치‥누구의 작품일까 2023.07.06뉴스데스크MBC.csv',encoding='cp949')\n",
        "docs18 = df18['comment']\n",
        "docs18\n",
        "\n",
        "\n",
        "df20 = pd.read_csv('/content/sample_data/20.창작하는 AI‥예술가는 살아남을 수 있을까 2023.02.20뉴스데스크MBC.csv',encoding='cp949')\n",
        "docs20 = df20['comment']\n",
        "docs20\n",
        "\n",
        "\n",
        "\n",
        "docs = pd.concat([docs1, docs2, docs3, docs4, docs5, docs6, docs7, docs8, docs9, docs10, docs11, docs12, docs13, docs14, docs15, docs16, docs17, docs18, docs20],ignore_index=True)\n"
      ],
      "metadata": {
        "id": "ocDaFFDJFixY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRYX0sLNQPF2",
        "outputId": "0c1a19fe-61b6-43c3-f052-111b48617a41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3959"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic[visualization]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65PP2OhzJgxe",
        "outputId": "abb1c3e8-29bc-4777-8e11-ed912a74ba8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic[visualization]\n",
            "  Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m122.9/143.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: bertopic 0.15.0 does not provide the extra 'visualization'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic[visualization]) (1.23.5)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic[visualization])\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic[visualization])\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic[visualization]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic[visualization]) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic[visualization]) (4.66.1)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic[visualization])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic[visualization]) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic[visualization])\n",
            "  Using cached Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic[visualization]) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic[visualization]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic[visualization]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic[visualization]) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic[visualization]) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic[visualization]) (23.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic[visualization]) (3.2.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.4.1->bertopic[visualization])\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic[visualization]) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic[visualization]) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic[visualization]) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic[visualization])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.4.1->bertopic[visualization])\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic[visualization]) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic[visualization])\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tbb>=2019.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic[visualization]) (2021.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (4.5.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[visualization]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[visualization]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic[visualization]) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[visualization])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[visualization])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic[visualization]) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic[visualization]) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic[visualization]) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[visualization]) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039168 sha256=6974e10c4d70342b27231dbaed6e363918fdffe47ab65e0ab10af3fe93bcd8fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=c684c54f4905f9a21c6df3ae127a35ddf15b232984e561183eb43128b0f95053\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86770 sha256=1e3b4571e31358cf7e5abe70e58da8a304e45f9012b807a69238115b0a2f3340\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/66/29/199acf5784d0f7b8add6d466175ab45506c96e386ed5dd0633\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=41f583164c6c69e314f7d6308feb3b38eaf371b481b58c38f81f92bfc0aa4f18\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, cython, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.2\n",
            "    Uninstalling Cython-3.0.2:\n",
            "      Successfully uninstalled Cython-3.0.2\n",
            "Successfully installed bertopic-0.15.0 cython-0.29.36 hdbscan-0.8.33 huggingface-hub-0.17.2 pynndescent-0.5.10 safetensors-0.3.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.2 umap-learn-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1abF9nwpRuS",
        "outputId": "a04c333c-5abc-4aba-ca82-59d35ea89b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Mecab-ko-for-Google-Colab"
      ],
      "metadata": {
        "id": "zFF_9gbj0BuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash install_mecab-ko_on_colab_light_220429.sh"
      ],
      "metadata": {
        "id": "64pSKpRB0GjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "UkvhsUlx0H0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt"
      ],
      "metadata": {
        "id": "sZZPFRmhpUMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 2016-10-20.txt"
      ],
      "metadata": {
        "id": "Qz9OzyLsqiRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "u4npCLv3qkbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from konlpy.tag import Mecab\n",
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "vY48-CNBqqDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "lHlxrqlOvLUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_documents = []\n",
        "\n",
        "for line in tqdm(docs):\n",
        "  # 빈 문자열이거나 숫자로만 이루어진 줄은 제외\n",
        "  if isinstance(line, str) and line.strip() and not line.replace(' ', '').isdigit():\n",
        "    preprocessed_documents.append(line)\n",
        "\n",
        "# preprocessed_documents 리스트에는 숫자가 제거된 텍스트 댓글만 포함됩니다.\n"
      ],
      "metadata": {
        "id": "TAX1VmG-uZua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, tagger):\n",
        "        self.tagger = tagger\n",
        "    def __call__(self, sent):\n",
        "        sent = sent[:1000000]\n",
        "        word_tokens = self.tagger.morphs(sent)\n",
        "        result = [word for word in word_tokens if len(word) > 1]\n",
        "        return result"
      ],
      "metadata": {
        "id": "dZXcopQIvNnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tokenizer = CustomTokenizer(Mecab())"
      ],
      "metadata": {
        "id": "8NKWub5iv3AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)"
      ],
      "metadata": {
        "id": "fDVdktkBvN8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTopic(embedding_model=\"sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\", \\\n",
        "                 vectorizer_model=vectorizer,\n",
        "                 nr_topics=50,\n",
        "                 top_n_words=50,\n",
        "                 calculate_probabilities=True)"
      ],
      "metadata": {
        "id": "g6u8n46zvPBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = model.fit_transform(preprocessed_documents)"
      ],
      "metadata": {
        "id": "opN1vEMTxKMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "5fvgH-t2yUkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_distribution(probs[0])"
      ],
      "metadata": {
        "id": "pYsuOmkR21Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 50):\n",
        "  print(i,'번째 토픽 :', model.get_topic(i))"
      ],
      "metadata": {
        "id": "sEys7_us21xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic"
      ],
      "metadata": {
        "id": "nMgTgbzf28CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "Y5Apw5MS4uPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTopic()\n",
        "topics, probabilities = model.fit_transform(preprocessed_documents)"
      ],
      "metadata": {
        "id": "2eb4LCT15Jdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('각 문서의 토픽 번호 리스트 :',len(topics))\n",
        "print('첫번째 문서의 토픽 번호 :', topics[0])"
      ],
      "metadata": {
        "id": "Uaj7hQQw5Nty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic_info()"
      ],
      "metadata": {
        "id": "2oRyKG6952Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic_info()['Count'].sum()"
      ],
      "metadata": {
        "id": "W97ryhFP52pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_topic(5)"
      ],
      "metadata": {
        "id": "1mmuHbvF57As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "Uo6nz41X59Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "Rz79tfif6BXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTopic(nr_topics=20)\n",
        "topics, probabilities = model.fit_transform(preprocessed_documents)\n",
        "\n",
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "npD_JdZA6K1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTopic(nr_topics=\"auto\")\n",
        "topics, probabilities = model.fit_transform(docs)\n",
        "\n",
        "model.get_topic_info()"
      ],
      "metadata": {
        "id": "CBrHqU646brV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc = docs[0]\n",
        "print(new_doc)"
      ],
      "metadata": {
        "id": "JsADnVLy7J2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = model.transform([new_doc])\n",
        "print('예측한 토픽 번호 :', topics)"
      ],
      "metadata": {
        "id": "xa6rdMw57NLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2stPfAB7PgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}