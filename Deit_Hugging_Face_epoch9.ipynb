{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMioH5vAS6qSw/6HuaO+af",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoon-Joon-soo/Kobert_for_ai/blob/master/Deit_Hugging_Face_epoch9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_pYH2CN8hzmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5beedc6-1670-4b30-dc44-b2ebda9cda59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZsqlnDHiIK4",
        "outputId": "ee242c4d-a4cc-4115-9aaa-a667b38a1b7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvXvDJvtnZhZ",
        "outputId": "2ecf9e18-ed2a-4fd0-d126-d6b9678bdfaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://huggingface.co/facebook/deit-base-distilled-patch16.git"
      ],
      "metadata": {
        "id": "stL9m_BkmbhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z8KrjatxLfG",
        "outputId": "f12f0ac0-e844-4468-f47e-09d2f60dc2a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import DeiTForImageClassification, DeiTFeatureExtractor\n",
        "\n",
        "model_name = \"facebook/deit-base-distilled-patch16-224\"  # 사용할 DeiT 모델의 이름\n",
        "access_token = \"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\"  # 생성한 Access Token\n",
        "\n",
        "# 모델을 로드할 때 token 매개변수를 사용하여 인증합니다.\n",
        "model = DeiTForImageClassification.from_pretrained(model_name, token=access_token)\n",
        "feature_extractor = DeiTFeatureExtractor.from_pretrained(model_name, token=access_token)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "nGwssPp6i5B0",
        "outputId": "05ce63b8-2c84-49ca-95c6-34ceb20f74ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import DeiTForImageClassification, DeiTFeatureExtractor\\n\\nmodel_name = \"facebook/deit-base-distilled-patch16-224\"  # 사용할 DeiT 모델의 이름\\naccess_token = \"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\"  # 생성한 Access Token\\n\\n# 모델을 로드할 때 token 매개변수를 사용하여 인증합니다.\\nmodel = DeiTForImageClassification.from_pretrained(model_name, token=access_token)\\nfeature_extractor = DeiTFeatureExtractor.from_pretrained(model_name, token=access_token)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYrWEsKgDh35",
        "outputId": "f449c6d4-e1c6-4a65-a5ad-548fcc0c9d6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHrdInHuLvWg",
        "outputId": "730890e9-0e89-45a7-8ea5-3eddfcabc337"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# Google 드라이브 공유 링크에서 파일 ID 추출 (공유 가능한 링크를 사용하면 \"uc?id=\" 이후의 부분이 파일 ID입니다)\n",
        "file_id = '1FmNWMIfgTrWbimQbAqFaSjpEcN_rChxp'\n",
        "\n",
        "# 다운로드할 파일의 링크 생성\n",
        "file_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# 파일 다운로드\n",
        "output_path = 'FERPlus_Dataset.zip'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# 압축 해제\n",
        "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('FERPlus_Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvgPA4z_D_KH",
        "outputId": "6ba026f0-9e39-4940-a29e-75e657518a5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1FmNWMIfgTrWbimQbAqFaSjpEcN_rChxp\n",
            "From (redirected): https://drive.google.com/uc?id=1FmNWMIfgTrWbimQbAqFaSjpEcN_rChxp&confirm=t&uuid=9b74eccb-89c4-4a71-baa9-db1e925c1244\n",
            "To: /content/FERPlus_Dataset.zip\n",
            "100%|██████████| 62.9M/62.9M [00:01<00:00, 36.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Data_loader.py\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def load_train_data(train_data_dir, pixel_size, batch_size):\n",
        "    # 훈련 데이터에 대한 데이터 변환(Transformation) 설정\n",
        "    train_data_transform = transforms.Compose([\n",
        "        transforms.Resize((pixel_size, pixel_size)),  # 이미지 크기를 48x48로 변경\n",
        "        transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\n",
        "    ])\n",
        "\n",
        "    # ImageFolder를 사용하여 훈련 데이터셋을 불러옵니다.\n",
        "    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=train_data_transform)\n",
        "\n",
        "    # DataLoader를 사용하여 훈련 데이터셋을 배치 단위로 로드합니다.\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(f\"클래스 개수: {len(train_dataset.classes)}\")\n",
        "\n",
        "    return train_data_loader\n",
        "\n",
        "def class_num(train_data_dir, pixel_size):\n",
        "    train_data_transform = transforms.Compose([\n",
        "        transforms.Resize((pixel_size, pixel_size)),  # 이미지 크기를 48x48로 변경\n",
        "        transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=train_data_transform)\n",
        "\n",
        "    return len(train_dataset.classes)\n",
        "\n",
        "def load_test_data(test_data_dir, pixel_size, batch_size):\n",
        "    # 검증 데이터에 대한 데이터 변환(Transformation) 설정\n",
        "    test_data_transform = transforms.Compose([\n",
        "        transforms.Resize((pixel_size, pixel_size)),  # 이미지 크기를 48x48로 변경\n",
        "        transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\n",
        "    ])\n",
        "\n",
        "    # ImageFolder를 사용하여 검증 데이터셋을 불러옵니다.\n",
        "    test_dataset = datasets.ImageFolder(root=test_data_dir, transform=test_data_transform)\n",
        "\n",
        "    # DataLoader를 사용하여 검증 데이터셋을 배치 단위로 로드합니다.\n",
        "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"클래스 개수: {len(test_dataset.classes)}\")\n",
        "\n",
        "    return test_data_loader\n",
        "'''"
      ],
      "metadata": {
        "id": "WHicwcPw_cwh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "8ff43bad-7ce6-4454-d3f0-df24d78b584f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Data_loader.py\\n\\n\\nimport torch\\nfrom torchvision import datasets, transforms\\n\\ndef load_train_data(train_data_dir, pixel_size, batch_size):\\n    # 훈련 데이터에 대한 데이터 변환(Transformation) 설정\\n    train_data_transform = transforms.Compose([\\n        transforms.Resize((pixel_size, pixel_size)),  # 이미지 크기를 48x48로 변경\\n        transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\\n    ])\\n\\n    # ImageFolder를 사용하여 훈련 데이터셋을 불러옵니다.\\n    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=train_data_transform)\\n\\n    # DataLoader를 사용하여 훈련 데이터셋을 배치 단위로 로드합니다.\\n    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\n\\n    print(f\"클래스 개수: {len(train_dataset.classes)}\")\\n\\n    return train_data_loader\\n\\ndef class_num(train_data_dir, pixel_size):\\n    train_data_transform = transforms.Compose([\\n        transforms.Resize((pixel_size, pixel_size)),  # 이미지 크기를 48x48로 변경\\n        transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\\n    ])\\n    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=train_data_transform)\\n\\n    return len(train_dataset.classes)\\n\\ndef load_test_data(test_data_dir, pixel_size, batch_size):\\n    # 검증 데이터에 대한 데이터 변환(Transformation) 설정\\n    test_data_transform = transforms.Compose([\\n        transforms.Resize((pixel_size, pixel_size)),  # 이미지 크기를 48x48로 변경\\n        transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\\n    ])\\n\\n    # ImageFolder를 사용하여 검증 데이터셋을 불러옵니다.\\n    test_dataset = datasets.ImageFolder(root=test_data_dir, transform=test_data_transform)\\n\\n    # DataLoader를 사용하여 검증 데이터셋을 배치 단위로 로드합니다.\\n    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n\\n    print(f\"클래스 개수: {len(test_dataset.classes)}\")\\n\\n    return test_data_loader\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data_loader.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class FERPlusDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, train=True, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.data_dir = os.path.join(self.root_dir, 'train')\n",
        "        else:\n",
        "            self.data_dir = os.path.join(self.root_dir, 'test')\n",
        "\n",
        "        self.classes = sorted(os.listdir(self.data_dir))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        self.samples = []\n",
        "        for cls_name in self.classes:\n",
        "            cls_dir = os.path.join(self.data_dir, cls_name)\n",
        "            for img_name in os.listdir(cls_dir):\n",
        "                img_path = os.path.join(cls_dir, img_name)\n",
        "                self.samples.append((img_path, self.class_to_idx[cls_name]))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, target = self.samples[idx]\n",
        "        img = Image.open(img_path).convert('RGB')  # 허깅 페이스에서 제공하는 deit 라이브러리가 컬러 이미지만을 취급하기 때문에 컬러로 변경\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n"
      ],
      "metadata": {
        "id": "I3rxgSlZcuwU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data_preprosessing.py\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# cv2 모듈로 이미지 읽는 함수 정의\n",
        "class compute_mean_and_std():\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths = glob.glob(os.path.join(root_dir, '**/*.*'), recursive=True)\n",
        "\n",
        "    def read_img(self, file_path):\n",
        "        img_arr = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "        return img_arr\n",
        "\n",
        "    def get_mean_and_std(self):\n",
        "        # global mean 구하기\n",
        "        global_mean = 0\n",
        "        global_var = 0\n",
        "\n",
        "        for img in self.image_paths:\n",
        "            img_result = self.read_img(img) / 255\n",
        "            global_mean += img_result.reshape(-1, 3).mean(axis=0)\n",
        "\n",
        "        global_mean /= len(self.image_paths)\n",
        "\n",
        "        # global std 구하기\n",
        "        for img in self.image_paths:\n",
        "            img_result = self.read_img(img) / 255\n",
        "            global_var += ((img_result.reshape(-1, 3) - global_mean) ** 2).mean(axis=0)\n",
        "\n",
        "        global_var /= len(self.image_paths)\n",
        "        global_std = np.sqrt(global_var)\n",
        "\n",
        "        print(global_mean, global_std)\n",
        "\n",
        "        return global_mean, global_std"
      ],
      "metadata": {
        "id": "BsXzEmntc2k7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine_tuning.py\n",
        "\n",
        "from torch import torch\n",
        "\n",
        "\n",
        "def fine_tuning(model, num_classes, num_epochs, train_data_loader):\n",
        "    model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes)\n",
        "\n",
        "    learning_rate = 0.001\n",
        "\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    i=1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            i = i+1\n",
        "            if(i%225==0):\n",
        "              print(f'This is {i/225}th iteration for fine-tuning.\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 검증 데이터에 모델 적용 및 성능 평가\n",
        "    # (정확도, F1 점수 등)\n",
        "    #model.eval()\n",
        "    #with torch.no_grad():\n",
        "        #torch.save(model.state_dict(), 'fine_tuned_model.pth')\n"
      ],
      "metadata": {
        "id": "QGo2fsT6c41L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_loader.py\n",
        "\n",
        "from transformers import DeiTForImageClassification\n",
        "\n",
        "def load_model(model_name):\n",
        "    model = DeiTForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mA9rEXToc56e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "\n",
        "class FERPlusDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, train=True, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.data_dir = os.path.join(self.root_dir, 'train')\n",
        "        else:\n",
        "            self.data_dir = os.path.join(self.root_dir, 'test')\n",
        "\n",
        "        self.classes = sorted(os.listdir(self.data_dir))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        self.samples = []\n",
        "        for cls_name in self.classes:\n",
        "            cls_dir = os.path.join(self.data_dir, cls_name)\n",
        "            for img_name in os.listdir(cls_dir):\n",
        "                img_path = os.path.join(cls_dir, img_name)\n",
        "                self.samples.append((img_path, self.class_to_idx[cls_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, target = self.samples[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# FERPlus 데이터셋 로드\n",
        "\n",
        "root_dir = 'FERPlus_Dataset'  # 압축 해제한 데이터셋의 경로로 설정하세요.\n",
        "train_dataset = FERPlusDataset(root_dir, train=True, transform=data_transform)\n",
        "test_dataset = FERPlusDataset(root_dir, train=False, transform=data_transform)\n",
        "\n",
        "\n",
        "# 데이터 로더 설정 (배치 크기, 셔플 등을 설정하세요)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "'''"
      ],
      "metadata": {
        "id": "bXieduRINrJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e537166f-ebdf-429b-c8aa-a57d88867162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "#fine_tuning.py\n",
        "\n",
        "from torch import torch\n",
        "\n",
        "\n",
        "def fine_tuning(model, num_classes, num_epochs, train_data_loader):\n",
        "    model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes)\n",
        "\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    i=0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f'i의 값은 {i}입니다')\n",
        "            i= i+1\n",
        "\n",
        "    # 검증 데이터에 모델 적용 및 성능 평가\n",
        "    # (정확도, F1 점수 등)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        torch.save(model.state_dict(), 'fine_tuned_model.pth')"
      ],
      "metadata": {
        "id": "rR4LEWJtXtTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_model.py\n",
        "\n",
        "from torch import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_model(model, test_data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    j=1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_data_loader:\n",
        "            outputs = model(inputs).logits  # ImageClassifierOutput에서 logits 텐서 추출\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    print('Test Finish!!')\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "K17VDvEbPiLG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "from torch import torch\n",
        "\n",
        "\n",
        "root_dir = '/content/FERPlus_Dataset'\n",
        "batch_size = 128\n",
        "model_name = \"facebook/deit-base-distilled-patch16-224\"\n",
        "\n",
        "# 이미지 크기 조정 변환\n",
        "desired_size = 224\n",
        "resize_transform= transforms.Resize((desired_size,desired_size))\n",
        "\n",
        "desired_padding = 88\n",
        "# 패딩을 추가하는 과정+\n",
        "padding_transform = transforms.Compose([\n",
        "    #transforms.ToPILImage(), # Tensor를 PIL 이미지로 변환\n",
        "    transforms.Pad(padding=desired_padding, fill=0, padding_mode= 'constant'), # desired_padding에 맞게 패딩 추가\n",
        "])\n",
        "\n",
        "# 데이터 정규화 즉, 평균과 표준편차 계산해서 반환하기\n",
        "mean_std_calculator = compute_mean_and_std(root_dir)\n",
        "mean, std = mean_std_calculator.get_mean_and_std()\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "data_transform = transforms.Compose([\n",
        "    padding_transform, # padding\n",
        "    resize_transform,  # resizing\n",
        "    transforms.Grayscale(num_output_channels=3),  # 이미지를 흑백에서 RGB로 변환\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# FERPlus 데이터 셋 로드\n",
        "train_dataset = FERPlusDataset(root_dir, train=True, transform=data_transform)\n",
        "test_dataset = FERPlusDataset(root_dir, train=False, transform=data_transform)\n",
        "\n",
        "# 데이터 로더 설정 (배치 크기, 셔플 등을 설정하세요)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "print(f\"class number: {len(train_dataset.classes)}\")\n",
        "print(f\"data number: {len(train_dataset)}\")\n",
        "print(f\"batch number: {len(train_loader)}\")\n",
        "\n",
        "\n",
        "# 모델 로드\n",
        "model = load_model(model_name)\n",
        "\n",
        "# 저장된 모델 가중치 불러오기\n",
        "model_state_dict = torch.load('/content/drive/MyDrive/model_weights_after_9epochs.pth')\n",
        "\n",
        "# 모델의 classifier 레이어를 변경하여 클래스 수 조정\n",
        "num_classes = len(train_dataset.classes)\n",
        "model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes)\n",
        "\n",
        "# 모델의 가중치를 불러옴\n",
        "model.load_state_dict(model_state_dict, strict=True)\n",
        "\n",
        "# 추가 에폭 학습\n",
        "num_epochs_additional = 1\n",
        "fine_tuning(model, num_classes, num_epochs_additional, train_loader)\n",
        "\n",
        "# 저장된 모델 가중치 저장\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/model_weights_after_10epochs.pth')\n",
        "\n",
        "# 테스트\n",
        "accuracy = test_model(model, test_loader)\n",
        "print(f\"Test Accuracy after {num_epochs_additional} additional epochs: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw3bX1swPjPV",
        "outputId": "f943823f-ff44-4585-a78a-8465b4a31000"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.50739176 0.50739176 0.50739176] [0.25519799 0.25519799 0.25519799]\n",
            "class number: 7\n",
            "data number: 28709\n",
            "batch number: 225\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is 1.0th iteration for fine-tuning.\n",
            "\n",
            "This is 2.0th iteration for fine-tuning.\n",
            "\n",
            "Test Finish!!\n",
            "Test Accuracy after 2 additional epochs: 0.47757035385901364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = load_model(model_name)\n",
        "\n",
        "# 저장된 모델 가중치 불러오기\n",
        "model_state_dict = torch.load('model_weights_after_first_epochs.pth')\n",
        "\n",
        "# 모델의 classifier 레이어를 변경하여 클래스 수 조정\n",
        "num_classes = len(train_dataset.classes)\n",
        "model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=num_classes)\n",
        "\n",
        "# 모델의 가중치를 불러옴\n",
        "model.load_state_dict(model_state_dict, strict=False)\n",
        "\n",
        "# 추가 7 에폭 학습\n",
        "num_epochs_additional = 7\n",
        "fine_tuning(model, num_classes, num_epochs_additional, train_loader)\n",
        "\n",
        "# 테스트\n",
        "accuracy = test_model(model, test_loader)\n",
        "print(f\"Test Accuracy after {num_epochs_additional} additional epochs: {accuracy}\")\n",
        "\n",
        "# 중간 결과물 저장\n",
        "intermediate_weights_path = f'intermediate_model_weights_after_{num_epochs_additional}_epochs.pth'\n",
        "torch.save(model.state_dict(), intermediate_weights_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYdGuRWbdSJy",
        "outputId": "0d6ce2b9-f14b-4c49-aae1-9765f9da9584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#num_epochs = 7\n",
        "# 언제 든지 늘려도 됨. 일단은 임시로 아무 숫자나 설정해 둔 것.\n",
        "\n",
        "# 분류 작업을 위한 fine-tuning 레이어 설정\n",
        "#fine_tuning(model, len(train_dataset.classes), num_epochs, train_loader)\n",
        "\n",
        "# 저장된 모델 가중치 저장\n",
        "#torch.save(model.state_dict(), '/content/drive/MyDrive/model_weights_after_epochs.pth')\n",
        "\n",
        "#accuracy = test_model(model, test_loader)\n",
        "#print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "OPCzbE0CeUuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}